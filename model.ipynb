{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a449e721",
   "metadata": {},
   "source": [
    "1. Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ee2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b468bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    emb_dim: int\n",
    "    hidden_dim: int\n",
    "    dtype: any = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x_fc1 = nn.Dense(self.hidden_dim, use_bias=False, dtype=self.dtype)(x)\n",
    "        x_fc2 = nn.Dense(self.hidden_dim, use_bias=False, dtype=self.dtype)(x)\n",
    "        x = nn.gelu(x_fc1, approximate=True) * x_fc2\n",
    "        x = nn.Dense(self.emb_dim, use_bias=False, dtype=self.dtype)(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abde91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    emb_dim: int\n",
    "    eps: float = 1e-6\n",
    "    bias: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Compute RMSNorm\n",
    "        rms = jnp.sqrt(jnp.mean(x**2, axis=-1, keepdims=True) + self.eps)\n",
    "        x_norm = x / rms\n",
    "\n",
    "        scale = self.param('scale', nn.initializers.zeros, (self.emb_dim,))\n",
    "        scale = 1.0 + scale  # Match Gemma3's (1 + weight) scaling\n",
    "        x_norm = x_norm * scale\n",
    "\n",
    "        if self.bias:\n",
    "            shift = self.param('shift', nn.initializers.zeros, (self.emb_dim,))\n",
    "            x_norm = x_norm + shift\n",
    "\n",
    "        return x_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b882cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=jnp.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    inv_freq = 1.0 / ((theta_base) ** (jnp.arange(0, head_dim, 2, dtype=dtype) / head_dim))\n",
    "    positions = jnp.arange(context_length)\n",
    "    angles = positions[:, None] * inv_freq[None, :]\n",
    "    angles = jnp.concatenate([angles, angles], axis=-1)\n",
    "    cos = jnp.cos(angles)\n",
    "    sin = jnp.sin(angles)\n",
    "    return cos, sin\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "    x1, x2 = jnp.split(x, 2, axis=4)\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "    x1 = x1 * cos + x2 * sin\n",
    "    x2 = x2 * cos - x1 * sin\n",
    "    return jnp.concatenate([x1, x2], axis=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c260ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    d_in: int\n",
    "    num_heads: int\n",
    "    num_kv_groups: int\n",
    "    head_dim: int = None\n",
    "    qk_norm: bool = False\n",
    "    query_pre_attn_scalar: float = None\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, cos, sin):\n",
    "        assert self.num_heads % self.num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "        head_dim = self.head_dim or (self.d_in // self.num_heads)\n",
    "        d_out = self.num_heads * head_dim\n",
    "\n",
    "        W_query = nn.Dense(d_out, use_bias=False, dtype=self.dtype)\n",
    "        W_key = nn.Dense(self.num_kv_groups * head_dim, use_bias=False, dtype=self.dtype)\n",
    "        W_value = nn.Dense(self.num_kv_groups * head_dim, use_bias=False, dtype=self.dtype)\n",
    "        out_proj = nn.Dense(self.d_in, use_bias=False, dtype=self.dtype)\n",
    "\n",
    "        q = W_query(x)\n",
    "        k = W_key(x)\n",
    "        v = W_value(x)\n",
    "        out = out_proj(q)  # This is just a placeholder\n",
    "\n",
    "        if self.qk_norm:\n",
    "            q = nn.normalize(q, dim=-1)\n",
    "            k = nn.normalize(k, dim=-1)\n",
    "        \n",
    "        queries = apply_rope(q, cos, sin)\n",
    "        keys = apply_rope(k, cos, sin)\n",
    "\n",
    "        \n",
    "        keys = jnp.repeat(keys, self.group_size, axis=1)\n",
    "        values = jnp.repeat(values, self.group_size, axis=1)\n",
    "\n",
    "\n",
    "        return x  # Replace with actual attention logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2be28dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, attn_type):\n",
    "        super().__init__()\n",
    "        self.attn_type = attn_type \n",
    "\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            qk_norm=cfg[\"qk_norm\"],\n",
    "            query_pre_attn_scalar=cfg[\"query_pre_attn_scalar\"],\n",
    "            dtype=cfg[\"dtype\"],\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.input_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.post_attention_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.pre_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.post_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "        x,\n",
    "        mask_global,\n",
    "        mask_local,\n",
    "        cos_global,\n",
    "        sin_global,\n",
    "        cos_local,\n",
    "        sin_local,\n",
    "    ):\n",
    "        shortcut = x\n",
    "        x = self.input_layernorm(x)\n",
    "\n",
    "        if self.attn_type == \"sliding_attention\":\n",
    "            attn_mask = mask_local\n",
    "            cos = cos_local\n",
    "            sin = sin_local\n",
    "        else:\n",
    "            attn_mask = mask_global\n",
    "            cos = cos_global\n",
    "            sin = sin_global\n",
    "        \n",
    "        x_attn = self.att(x, attn_mask, cos, sin)\n",
    "        x_attn = self.post_attention_layernorm(x_attn)\n",
    "        x = shortcut + x_attn\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x_ffn = self.pre_feedforward_layernorm(x)\n",
    "        x_ffn = self.ff(x_ffn)\n",
    "        x_ffn = self.post_feedforward_layernorm(x_ffn)\n",
    "        x = shortcut + x_ffn\n",
    "        return x\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5086ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "class Gemma3Model(nn.Module):\n",
    "    cfg: Dict[str, Any]\n",
    "\n",
    "    def setup(self):\n",
    "        # --- Assertions and config checks ---\n",
    "        assert self.cfg[\"layer_types\"] is not None and len(self.cfg[\"layer_types\"]) == self.cfg[\"n_layers\"]\n",
    "\n",
    "        # --- Token Embedding ---\n",
    "        self.tok_emb = nn.Embed(\n",
    "            num_embeddings=self.cfg[\"vocab_size\"],\n",
    "            features=self.cfg[\"emb_dim\"],\n",
    "            dtype=self.cfg[\"dtype\"],\n",
    "        )\n",
    "\n",
    "        # --- Transformer blocks ---\n",
    "        self.blocks = [\n",
    "            TransformerBlock(self.cfg, attn_type)\n",
    "            for attn_type in self.cfg[\"layer_types\"]\n",
    "        ]\n",
    "\n",
    "        # --- Final normalization ---\n",
    "        self.final_norm = RMSNorm(\n",
    "            dim=self.cfg[\"emb_dim\"], eps=1e-6\n",
    "        )\n",
    "\n",
    "        # --- Output projection (logits) ---\n",
    "        self.out_head = nn.Dense(\n",
    "            features=self.cfg[\"vocab_size\"],\n",
    "            use_bias=False,\n",
    "            dtype=self.cfg[\"dtype\"],\n",
    "        )\n",
    "\n",
    "        # --- RoPE params (cos, sin) ---\n",
    "        cos_local, sin_local = compute_rope_params(\n",
    "            head_dim=self.cfg[\"head_dim\"],\n",
    "            theta_base=self.cfg[\"rope_local_base\"],\n",
    "            context_length=self.cfg[\"context_length\"],\n",
    "            dtype=jnp.float32,\n",
    "        )\n",
    "        cos_global, sin_global = compute_rope_params(\n",
    "            head_dim=self.cfg[\"head_dim\"],\n",
    "            theta_base=self.cfg[\"rope_base\"],\n",
    "            context_length=self.cfg[\"context_length\"],\n",
    "            dtype=jnp.float32,\n",
    "        )\n",
    "\n",
    "        # In Flax, store as \"constants\" (non-trainable variables)\n",
    "        self.cos_local = self.variable(\"constants\", \"cos_local\", lambda: cos_local)\n",
    "        self.sin_local = self.variable(\"constants\", \"sin_local\", lambda: sin_local)\n",
    "        self.cos_global = self.variable(\"constants\", \"cos_global\", lambda: cos_global)\n",
    "        self.sin_global = self.variable(\"constants\", \"sin_global\", lambda: sin_global)\n",
    "\n",
    "    def _create_masks(self, seq_len: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        ones = jnp.ones((seq_len, seq_len), dtype=bool)\n",
    "\n",
    "        # Global causal mask (upper triangular, future is masked)\n",
    "        mask_global = jnp.triu(ones, k=1)\n",
    "\n",
    "        # Far-past mask (sliding window)\n",
    "        far_past = jnp.triu(ones, k=self.cfg[\"sliding_window\"]).T\n",
    "\n",
    "        # Local mask = future OR far past\n",
    "        mask_local = jnp.logical_or(mask_global, far_past)\n",
    "\n",
    "        return mask_global, mask_local\n",
    "\n",
    "    def __call__(self, input_ids: jnp.ndarray) -> jnp.ndarray:\n",
    "        # Batch size, sequence length\n",
    "        b, seq_len = input_ids.shape\n",
    "\n",
    "        # Token embedding + scaling\n",
    "        x = self.tok_emb(input_ids) * (self.cfg[\"emb_dim\"] ** 0.5)\n",
    "\n",
    "        # Build masks\n",
    "        mask_global, mask_local = self._create_masks(seq_len)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(\n",
    "                x,\n",
    "                mask_global=mask_global,\n",
    "                mask_local=mask_local,\n",
    "                cos_global=self.cos_global.value,\n",
    "                sin_global=self.sin_global.value,\n",
    "                cos_local=self.cos_local.value,\n",
    "                sin_local=self.sin_local.value,\n",
    "            )\n",
    "\n",
    "        # Normalize and project to vocab logits\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.astype(self.cfg[\"dtype\"]))\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98fb2bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMMA3_CONFIG_270M = {\n",
    "    \"vocab_size\": 262_144,\n",
    "    \"context_length\": 32_768,\n",
    "    \"emb_dim\": 640,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 18,\n",
    "    \"hidden_dim\": 2048,\n",
    "    \"head_dim\": 256,\n",
    "    \"qk_norm\": True,\n",
    "    \"n_kv_groups\": 1,\n",
    "    \"rope_local_base\": 10_000.0,\n",
    "    \"rope_base\": 1_000_000.0,\n",
    "    \"sliding_window\": 512,\n",
    "      \"layer_types\": [\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"sliding_attention\",\n",
    "        \"full_attention\"\n",
    "    ],\n",
    "    \"dtype\": jnp.bfloat16,\n",
    "    \"query_pre_attn_scalar\": 256,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aa35a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3Model(\n",
       "    # attributes\n",
       "    cfg = {'vocab_size': 262144, 'context_length': 32768, 'emb_dim': 640, 'n_heads': 4, 'n_layers': 18, 'hidden_dim': 2048, 'head_dim': 256, 'qk_norm': True, 'n_kv_groups': 1, 'rope_local_base': 10000.0, 'rope_base': 1000000.0, 'sliding_window': 512, 'layer_types': ['sliding_attention', 'sliding_attention', 'sliding_attention', 'sliding_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'sliding_attention', 'sliding_attention', 'sliding_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'sliding_attention', 'sliding_attention', 'sliding_attention', 'sliding_attention', 'full_attention'], 'dtype': <class 'jax.numpy.bfloat16'>, 'query_pre_attn_scalar': 256}\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Gemma3Model(GEMMA3_CONFIG_270M)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba372912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
