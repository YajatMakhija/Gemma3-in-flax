{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a449e721",
   "metadata": {},
   "source": [
    "1. Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ee2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b468bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    emb_dim: int\n",
    "    hidden_dim: int\n",
    "    dtype: any = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x_fc1 = nn.Dense(self.hidden_dim, use_bias=False, dtype=self.dtype)(x)\n",
    "        x_fc2 = nn.Dense(self.hidden_dim, use_bias=False, dtype=self.dtype)(x)\n",
    "        x = nn.gelu(x_fc1, approximate=True) * x_fc2\n",
    "        x = nn.Dense(self.emb_dim, use_bias=False, dtype=self.dtype)(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abde91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMNSNorm(nn.Module):\n",
    "    emb_dim: int\n",
    "    eps: float = 1e-6\n",
    "    bias: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Compute RMSNorm\n",
    "        rms = jnp.sqrt(jnp.mean(x**2, axis=-1, keepdims=True) + self.eps)\n",
    "        x_norm = x / rms\n",
    "\n",
    "        scale = self.param('scale', nn.initializers.zeros, (self.emb_dim,))\n",
    "        scale = 1.0 + scale  # Match Gemma3's (1 + weight) scaling\n",
    "        x_norm = x_norm * scale\n",
    "\n",
    "        if self.bias:\n",
    "            shift = self.param('shift', nn.initializers.zeros, (self.emb_dim,))\n",
    "            x_norm = x_norm + shift\n",
    "\n",
    "        return x_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b882cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=jnp.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    inv_freq = 1.0 / ((theta_base) ** (jnp.arange(0, head_dim, 2, dtype=dtype) / head_dim))\n",
    "    positions = jnp.arange(context_length)\n",
    "    angles = positions[:, None] * inv_freq[None, :]\n",
    "    angles = jnp.concatenate([angles, angles], axis=-1)\n",
    "    cos = jnp.cos(angles)\n",
    "    sin = jnp.sin(angles)\n",
    "    return cos, sin\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "    x1, x2 = jnp.split(x, 2, axis=4)\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "    x1 = x1 * cos + x2 * sin\n",
    "    x2 = x2 * cos - x1 * sin\n",
    "    return jnp.concatenate([x1, x2], axis=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c260ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    d_in: int\n",
    "    num_heads: int\n",
    "    num_kv_groups: int\n",
    "    head_dim: int = None\n",
    "    qk_norm: bool = False\n",
    "    query_pre_attn_scalar: float = None\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, cos, sin):\n",
    "        assert self.num_heads % self.num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "        head_dim = self.head_dim or (self.d_in // self.num_heads)\n",
    "        d_out = self.num_heads * head_dim\n",
    "\n",
    "        W_query = nn.Dense(d_out, use_bias=False, dtype=self.dtype)\n",
    "        W_key = nn.Dense(self.num_kv_groups * head_dim, use_bias=False, dtype=self.dtype)\n",
    "        W_value = nn.Dense(self.num_kv_groups * head_dim, use_bias=False, dtype=self.dtype)\n",
    "        out_proj = nn.Dense(self.d_in, use_bias=False, dtype=self.dtype)\n",
    "\n",
    "        q = W_query(x)\n",
    "        k = W_key(x)\n",
    "        v = W_value(x)\n",
    "        out = out_proj(q)  # This is just a placeholder\n",
    "\n",
    "        if self.qk_norm:\n",
    "            q = nn.normalize(q, dim=-1)\n",
    "            k = nn.normalize(k, dim=-1)\n",
    "        \n",
    "        queries = apply_rope(q, cos, sin)\n",
    "        keys = apply_rope(k, cos, sin)\n",
    "\n",
    "        \n",
    "        keys = jnp.repeat(keys, self.group_size, axis=1)\n",
    "        values = jnp.repeat(values, self.group_size, axis=1)\n",
    "\n",
    "\n",
    "        return x  # Replace with actual attention logic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
